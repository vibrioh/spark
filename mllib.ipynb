{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 Part 1: Building an Explicit Movie Recommendation System with Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"recommendation_explicit\") \\\n",
    "    .config(\"spark.som.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Unzip the MovieLens 1M dataset from http://grouplens.org/datasets/movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call([\"wget\", \"http://files.grouplens.org/datasets/movielens/ml-1m.zip\"])\n",
    "subprocess.call([\"unzip\", \"ml-1m.zip\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Convert ratings data to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"./ml-1m/ratings.dat\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the number of ratings in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ratings = 1000209\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of ratings = \" + str(ratings.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a sample of the Ratings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+------+\n",
      "|movieId|rating|timestamp|userId|\n",
      "+-------+------+---------+------+\n",
      "|   2908|   5.0|977895809|    68|\n",
      "|   3730|   5.0|978554445|   173|\n",
      "|   2917|   2.0|976301830|   456|\n",
      "|    589|   4.0|976161565|   526|\n",
      "|   2348|   3.0|976207524|   533|\n",
      "|   1285|   4.0|979154572|   588|\n",
      "|   1206|   4.0|980628867|   711|\n",
      "|   3361|   4.0|975510209|   730|\n",
      "|   3203|   5.0|975435824|   779|\n",
      "|   1196|   4.0|975356701|   843|\n",
      "+-------+------+---------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.sample(False, 0.0001, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample number of ratings per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|userId|No. of ratings|\n",
      "+------+--------------+\n",
      "|    26|           400|\n",
      "|    29|           108|\n",
      "|   474|           318|\n",
      "|   964|            78|\n",
      "|  1677|            43|\n",
      "|  1697|           354|\n",
      "|  1806|           214|\n",
      "|  1950|           137|\n",
      "|  2040|            46|\n",
      "|  2214|            81|\n",
      "+------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_ratings = ratings.groupBy(\"userId\").count().withColumnRenamed(\"count\", \"No. of ratings\")\n",
    "grouped_ratings.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the number of users in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 6040\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users = \" + str(grouped_ratings.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Ratings data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Show resulting Ratings dataset counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ratings = 1000209\n",
      "Training dataset count = 800880, 80.07126510559293%\n",
      "Test dataset count = 199329, 19.928734894407068%\n"
     ]
    }
   ],
   "source": [
    "trainingRatio = float(training.count())/float(ratings.count())*100\n",
    "testRatio = float(test.count())/float(ratings.count())*100\n",
    "\n",
    "print(\"Total number of ratings = \" + str(ratings.count()))\n",
    "print(\"Training dataset count = \" + str(training.count()) + \", \" + str(trainingRatio) + \"%\")\n",
    "print(\"Test dataset count = \" + str(test.count()) + \", \" + str(testRatio) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the recommendation model on the training data using ALS-explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model against the Test data and show a sample of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+------+----------+\n",
      "|movieId|rating|timestamp|userId|prediction|\n",
      "+-------+------+---------+------+----------+\n",
      "|    148|   1.0|976295338|   840| 2.9349167|\n",
      "|    148|   2.0|974875106|  1150| 2.9894443|\n",
      "|    148|   2.0|974178993|  2456| 3.9975448|\n",
      "|    463|   5.0|968916009|  3151|  3.967182|\n",
      "|    463|   3.0|963746396|  4858| 2.0730953|\n",
      "|    463|   4.0|973625620|  2629| 3.1774714|\n",
      "|    463|   1.0|966523740|  3683| 1.1212827|\n",
      "|    463|   2.0|966790403|  3562|  2.780132|\n",
      "|    463|   4.0|975775726|   721| 3.3978982|\n",
      "|    463|   3.0|965308300|  4252| 0.9944763|\n",
      "+-------+------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test).na.drop()\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model by computing the RMSE on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.8908929362860674\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that a smaller value of rmse is better\n",
    "This is obviously the case since RMSE is an aggregation of all the error. Thus evaluator.isLargerBetter should be 'false'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.isLargerBetter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make movie recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample recommendations per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                               |\n",
      "+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|148   |[[1780,7.2854385], [1369,6.99533], [666,6.6703053], [2892,6.5549903], [1741,6.528875], [3523,6.07751], [572,6.003775], [2127,5.859668], [1164,5.6353364], [649,5.5918784]]    |\n",
      "|5173  |[[3245,7.7563887], [1038,7.52281], [3867,7.2047706], [632,7.0838833], [37,7.0073814], [751,6.936385], [1369,6.471981], [645,6.453275], [1664,6.23118], [1543,6.188328]]       |\n",
      "|5695  |[[1458,9.663776], [3855,9.074218], [3106,9.053921], [2837,9.043263], [2192,8.797422], [2397,8.7831135], [341,8.623167], [1511,8.611192], [3636,8.600376], [219,8.446543]]     |\n",
      "|1863  |[[962,6.392259], [2175,6.2921085], [2984,6.027778], [759,5.9641767], [3737,5.929455], [2284,5.917394], [2426,5.894059], [854,5.883927], [2209,5.847718], [2705,5.8452163]]    |\n",
      "|1924  |[[1038,8.618518], [219,7.9083204], [131,7.871811], [632,7.788521], [1458,7.681244], [1574,7.473834], [119,7.1832986], [1696,7.0665197], [1312,6.7171383], [1651,6.703975]]    |\n",
      "|4610  |[[3670,6.8609476], [1117,6.645418], [2994,6.6018786], [2830,6.596518], [2934,6.505612], [3851,6.3677354], [2512,6.349237], [106,6.3229113], [2933,6.315516], [96,6.302059]]   |\n",
      "|4104  |[[649,7.115762], [1421,6.597936], [3885,6.493393], [1585,6.441885], [1741,6.0131593], [503,5.8390074], [3847,5.8177996], [443,5.6730995], [2624,5.634462], [3749,5.602259]]   |\n",
      "|1249  |[[3636,8.443559], [1420,7.907082], [1664,7.8959613], [3456,7.7776465], [2697,7.7743106], [702,7.7192597], [2825,7.573881], [2933,7.547401], [3900,7.4978175], [645,7.3760333]]|\n",
      "|855   |[[3670,5.6403356], [557,5.452341], [503,4.9971642], [3338,4.9897413], [3012,4.9187536], [2830,4.8673472], [1664,4.8444023], [3851,4.840315], [572,4.8038983], [2934,4.760828]]|\n",
      "|5361  |[[3523,8.854711], [1662,7.23445], [2388,7.046192], [1780,6.9375844], [2892,6.929945], [2164,6.6275907], [1117,6.4804296], [1846,6.458152], [131,6.426946], [2962,6.3652472]]  |\n",
      "+------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs.sample(False, 0.01).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show sample recommendations per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|movieId|recommendations                                                                                                                                                                |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|3844   |[[1213,7.3201046], [2441,6.9640417], [5297,6.8789372], [2549,6.8698826], [2816,6.507644], [1971,6.458085], [2160,6.4162674], [3915,6.402381], [4544,6.17197], [2560,6.119645]] |\n",
      "|1031   |[[1070,5.9382234], [4143,5.8492775], [3897,5.841146], [2755,5.6947303], [4282,5.6827908], [527,5.6089225], [1728,5.5674863], [5052,5.52997], [5983,5.419548], [1459,5.4131107]]|\n",
      "|26     |[[1213,7.0531287], [2640,6.3756685], [879,6.1351347], [2502,6.0931673], [5298,5.9518814], [642,5.873951], [1808,5.86157], [6038,5.8189907], [2535,5.804851], [2755,5.7891493]] |\n",
      "|626    |[[4504,9.705521], [3222,8.426963], [1713,8.153491], [5863,7.892766], [4583,7.852765], [3113,7.608546], [4776,7.5394926], [206,7.5082846], [2713,7.271112], [4008,7.1363134]]   |\n",
      "|3752   |[[5670,6.538592], [21,5.9881763], [5258,5.949679], [4393,5.7138], [4028,5.6019115], [1025,5.459873], [5877,5.4184914], [87,5.411454], [2357,5.375736], [5462,5.3705955]]       |\n",
      "|2256   |[[745,7.8676734], [2469,7.4058766], [906,7.213084], [2431,7.1617584], [1754,7.1158795], [5030,7.11016], [3911,6.9476233], [527,6.4272637], [700,6.3252373], [1713,6.281575]]   |\n",
      "|3793   |[[640,5.7342196], [5218,5.440282], [1673,5.2526026], [947,5.2225814], [2694,5.2105126], [2879,5.199566], [768,5.188442], [115,5.168048], [527,5.159202], [4936,5.1525726]]     |\n",
      "|2867   |[[745,5.992924], [2534,5.8074617], [527,5.6805005], [2755,5.653826], [283,5.3882546], [3587,5.3234334], [3902,5.3050156], [246,5.27825], [5440,5.2693644], [3373,5.2450128]]   |\n",
      "|846    |[[4008,10.775237], [4504,10.658872], [3222,9.88133], [399,9.678963], [5240,9.402692], [144,9.301779], [653,9.236071], [734,9.1837225], [2191,8.888657], [1014,8.74862]]        |\n",
      "|729    |[[665,11.115968], [1459,9.497441], [5803,7.76634], [1384,7.726793], [4317,7.657247], [640,7.6146173], [4427,7.6077237], [3870,7.5751157], [3463,7.331725], [3186,6.939564]]    |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieRecs.sample(False, 0.01).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 Part 2: Building an Implicit Music Recommendation System with Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"recommendation_implicit\") \\\n",
    "    .config(\"spark.som.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Unzip Audioscrobbler dataset from http://www.iro.umontreal.ca/~lisa/datasets/profiledata_06-May-2005.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.call([\"wget\", \"http://www.iro.umontreal.ca/~lisa/datasets/profiledata_06-May-2005.tar.gz\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Convert ratings data to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"./profiledata_06-May-2005/user_artist_data.txt\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\" \")).filter(lambda line: line!=None)\n",
    "userArtistRDD = parts.map(lambda p: Row(userId=int(p[0]), artistId=int(p[1]),\n",
    "                                     count=int(p[2])))\n",
    "userArtist = spark.createDataFrame(userArtistRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the number of userArtist in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of userArtist = 24296858\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of userArtist = \" + str(userArtist.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a sample of the userArtist DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+\n|artistId|count| userId|\n+--------+-----+-------+\n| 1054292|    1|1000127|\n| 1033246|    1|1000215|\n|    1269|   13|1000357|\n|     630|    1|1000410|\n| 1000428|    2|1000657|\n| 1234327|    1|1000911|\n|      45|   34|1000923|\n|     969|    4|1000927|\n|    1235|    1|1000928|\n|    3950|    2|1001009|\n+--------+-----+-------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "userArtist.sample(False, 0.0001, seed=23).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split userArtist data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = userArtist.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show resulting userArtist dataset counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingRatio = float(training.count())/float(userArtist.count())*100\n",
    "testRatio = float(test.count())/float(userArtist.count())*100\n",
    "\n",
    "print(\"Total number of userArtist = \" + str(userArtist.count()))\n",
    "print(\"Training dataset count = \" + str(training.count()) + \", \" + str(trainingRatio) + \"%\")\n",
    "print(\"Test dataset count = \" + str(test.count()) + \", \" + str(testRatio) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the recommendation model on the training data using ALS-implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, implicitPrefs=True, userCol=\"userId\", itemCol=\"artistId\", ratingCol=\"count\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "# model.save(\"hdfs://localhost:9000/user/vibrioh/implicit_model\")\n",
    "sameModel = ALSModel.load(\"hdfs://localhost:9000/user/vibrioh/implicit_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model against the Test data and show a sample of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+----------+\n|artistId|count| userId|prediction|\n+--------+-----+-------+----------+\n|     463|    1|1000117| 1.0815843|\n|     463|    1|1000221| 0.3066332|\n|     463|    1|1000401|0.41309264|\n|     463|    1|1000463|0.92721707|\n|     463|    1|1000614|0.66710955|\n|     463|    1|1000745| 0.5759305|\n|     463|    1|1000771|0.10025656|\n|     463|    1|1000920|0.14627631|\n|     463|    1|1001192| 0.4186052|\n|     463|    1|1001239|0.11049299|\n+--------+-----+-------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "predictions = sameModel.transform(test).na.drop()\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show recommendations high and low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+----------+\n|artistId|count| userId|prediction|\n+--------+-----+-------+----------+\n|     393|   44|1044648|  2.048578|\n| 1002862|    3|1000764| 2.0438032|\n| 1245208|   16|1077252| 1.9122567|\n|    1457|    3|1052461| 1.8336266|\n|    4538|    1|1044648| 1.8334882|\n| 1003361|    8|1052461| 1.7632964|\n| 1105069|   12|1045876| 1.7556672|\n|     670|   22|2058707| 1.7466245|\n| 1034635|  208|1038380| 1.7438686|\n| 1043348|   56|1021501| 1.7395341|\n| 1003133|    3|1044648| 1.7320846|\n| 1299851|   55|1007308| 1.7286341|\n| 1296257|   27|1007308| 1.7270842|\n| 1034635|   54|1047668| 1.7216785|\n| 1031984|    3|1038826|  1.719785|\n| 1001169|    9|1072865| 1.7158957|\n|    2842|    3|1077252| 1.7109416|\n|    2017|  128|2089146|  1.698533|\n| 1012494|    5|1070844| 1.6947658|\n| 1034635|    4|1001562| 1.6939092|\n+--------+-----+-------+----------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+-----------+\n|artistId|count| userId| prediction|\n+--------+-----+-------+-----------+\n|     606|    1|1072273| -1.1034482|\n| 1062984|    5|1052461| -1.0605614|\n| 1002751|    1|2005325| -0.9165062|\n| 1000660|    1|2114152|-0.82818604|\n| 1000270|    1|1070177| -0.7904455|\n| 1001428|    1|2019216|-0.78354007|\n|    4569|    4|1072273| -0.7598609|\n| 1010728|   22|1054893| -0.7471355|\n| 1006594|    3|2231283| -0.7382127|\n| 1020783|    6|2200013| -0.6836228|\n|    1400|    4|2287446| -0.6611168|\n| 1003458|    1|1003897| -0.6341311|\n| 1007027|    8|1000647|  -0.631236|\n| 1003084|    1|1063655|-0.62958777|\n|    2138|    1|2147892| -0.6157164|\n|    1179|    2|1055807| -0.6106846|\n|    1223|    1|2269169| -0.6081734|\n| 1000418|    1|1020855| -0.5983083|\n| 1002451|    2|2216293| -0.5928024|\n| 1027267|    1|2200013|-0.58894813|\n+--------+-----+-------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "predictions.registerTempTable(\"predictions\")\n",
    "spark.sql(\"SELECT * FROM predictions ORDER BY prediction DESC\").show()\n",
    "spark.sql(\"SELECT * FROM predictions ORDER BY prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 Part 1: Clustering on News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download news articles from: https://www.kaggle.com/asad1m9a9h6mood/news-articles\n",
    "### This Dataset is scraped from https://www.thenews.com.pk website. It has news articles from 2015 till date related to business and sports. It Contains the Heading of the particular Article, Its content and its date. The content also contains the place from where the statement or Article was published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import BisectingKMeans, KMeans, GaussianMixture\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, NGram, IDF, StopWordsRemover\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"news_clustering\") \\\n",
    "    .config(\"spark.som.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KARACHI: The Sindh government has decided to b...</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>sindh govt decides to cut public transport far...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HONG KONG: Asian markets started 2015 on an up...</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>asia stocks up in new year trad</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HONG KONG:  Hong Kong shares opened 0.66 perce...</td>\n",
       "      <td>1/5/2015</td>\n",
       "      <td>hong kong stocks open 0.66 percent lower</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HONG KONG: Asian markets tumbled Tuesday follo...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>asian stocks sink euro near nine year</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK: US oil prices Monday slipped below $...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>us oil prices slip below 50 a barr</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KARACHI: The Sindh government has decided to b...</td>\n",
       "      <td>1/1/2015</td>\n",
       "      <td>sindh govt decides to cut public transport far...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HONG KONG: Asian markets started 2015 on an up...</td>\n",
       "      <td>1/2/2015</td>\n",
       "      <td>asia stocks up in new year trad</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HONG KONG:  Hong Kong shares opened 0.66 perce...</td>\n",
       "      <td>1/5/2015</td>\n",
       "      <td>hong kong stocks open 0.66 percent lower</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HONG KONG: Asian markets tumbled Tuesday follo...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>asian stocks sink euro near nine year</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK: US oil prices Monday slipped below $...</td>\n",
       "      <td>1/6/2015</td>\n",
       "      <td>us oil prices slip below 50 a barr</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"./Articles.csv\",sep=\",\", encoding = \"ISO-8859-1\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2692</td>\n",
       "      <td>2692</td>\n",
       "      <td>2692</td>\n",
       "      <td>2692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2584</td>\n",
       "      <td>666</td>\n",
       "      <td>2581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>strong&gt;TOKYO: Tokyo stocks climbed in early tr...</td>\n",
       "      <td>8/1/2016</td>\n",
       "      <td>Tokyo stocks open lower after BoJ under</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heading</th>\n",
       "      <th>NewsType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2692</td>\n",
       "      <td>2692</td>\n",
       "      <td>2692</td>\n",
       "      <td>2692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2584</td>\n",
       "      <td>666</td>\n",
       "      <td>2581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>strong&gt;TOKYO: Tokyo stocks climbed in early tr...</td>\n",
       "      <td>8/1/2016</td>\n",
       "      <td>Tokyo stocks open lower after BoJ under</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>1408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Article: string (nullable = true)\n |-- Date: string (nullable = true)\n |-- Heading: string (nullable = true)\n |-- NewsType: string (nullable = true)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------+\n|             Article|     Date|             Heading|NewsType|\n+--------------------+---------+--------------------+--------+\n|HONG KONG: Asian ...| 1/6/2015|asian stocks sink...|business|\n|ISLAMABAD: The Na...|1/23/2015|nepra prevents k ...|business|\n|ISLAMABAD: Pakist...|1/26/2015|pakistan fuel cri...|business|\n|ISLAMABAD: Federa...| 3/4/2015|pact with k elect...|business|\n|London: Oil price...| 3/6/2015|oil prices rise b...|business|\n+--------------------+---------+--------------------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame(data_df)\n",
    "data.printSchema()\n",
    "data.sample(False, 0.05).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Article\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol=\"filtered\")   \n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\",numFeatures=1048576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering by K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, kmeans])\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test and print interested columns of different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n|             Article|NewsType|prediction|\n+--------------------+--------+----------+\n|A major rally in ...|business|         1|\n|ATLANTA: Twelve P...|business|         1|\n|BEIJING: Pakistan...|business|         0|\n|Brussels: The EU ...|business|         0|\n|DUBAI: Talks betw...|business|         0|\n|HONG KONG: Hong K...|business|         0|\n|Hong Kong: Asian ...|business|         1|\n|Hong Kong: Asian ...|business|         1|\n|Hong Kong: Asian ...|business|         1|\n|Hong Kong: Asian ...|business|         1|\n+--------------------+--------+----------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n|             Article|NewsType|prediction|\n+--------------------+--------+----------+\n|AUCKLAND: Martin ...|  sports|         0|\n|Australia win run...|  sports|         0|\n|CAPE TOWN: Alex H...|  sports|         0|\n|CAPE TOWN: Captai...|  sports|         0|\n|CAPE TOWN: Poor w...|  sports|         0|\n|CAPE TOWN: Tiny T...|  sports|         0|\n|DHAKA: Bangladesh...|  sports|         0|\n|DHAKA: Bangladesh...|  sports|         0|\n|DHAKA: Captain of...|  sports|         0|\n|DHAKA: Hasan Mohs...|  sports|         0|\n+--------------------+--------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.registerTempTable(\"predictions\")\n",
    "spark.sql(\"SELECT Article, NewsType, prediction FROM predictions WHERE NewsType = 'business'\").show(10)\n",
    "spark.sql(\"SELECT Article, NewsType, prediction FROM predictions WHERE NewsType = 'sports'\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 Part 2: Clustering on Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import BisectingKMeans, KMeans, GaussianMixture\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, NGram, IDF, StopWordsRemover\n",
    "from pyspark.sql import Row\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wiki_clustering\") \\\n",
    "    .config(\"spark.som.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download/Unzip https://dumps.wikimedia.org/enwiki/20170920/enwiki-20170920-pages-articles14.xml-p7697599p7744799.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.call([\"wget\", \"https://dumps.wikimedia.org/enwiki/20170920/enwiki-20170920-pages-articles14.xml-p7697599p7744799.bz2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse xml to json using WikiExtractorw (https://github.com/attardi/wikiextractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subprocess.call([\"python3\", \"WikiExtractor.py\", \"-o wiki_extracted\", \"--json\", \"-b 230M\", \"/Users/vibrioh/Downloads/enwiki-20170920-pages-articles14.xml-p7697599p7744799.bz2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7697605</td>\n",
       "      <td>Konica Minolta Cup\\n\\nKonica Minolta Cup may r...</td>\n",
       "      <td>Konica Minolta Cup</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7697611</td>\n",
       "      <td>Archer (typeface)\\n\\nArcher is a slab serif ty...</td>\n",
       "      <td>Archer (typeface)</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7697612</td>\n",
       "      <td>Stockton Airport\\n\\nStockton Airport may refer...</td>\n",
       "      <td>Stockton Airport</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7697626</td>\n",
       "      <td>Ricky Minard\\n\\nRicky Donell Minard Jr. (born ...</td>\n",
       "      <td>Ricky Minard</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7697641</td>\n",
       "      <td>Alexander Peya\\n\\nAlexander Peya (born 27 June...</td>\n",
       "      <td>Alexander Peya</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7697605</td>\n",
       "      <td>Konica Minolta Cup\\n\\nKonica Minolta Cup may r...</td>\n",
       "      <td>Konica Minolta Cup</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7697611</td>\n",
       "      <td>Archer (typeface)\\n\\nArcher is a slab serif ty...</td>\n",
       "      <td>Archer (typeface)</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7697612</td>\n",
       "      <td>Stockton Airport\\n\\nStockton Airport may refer...</td>\n",
       "      <td>Stockton Airport</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7697626</td>\n",
       "      <td>Ricky Minard\\n\\nRicky Donell Minard Jr. (born ...</td>\n",
       "      <td>Ricky Minard</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7697641</td>\n",
       "      <td>Alexander Peya\\n\\nAlexander Peya (born 27 June...</td>\n",
       "      <td>Alexander Peya</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7697641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/vibrioh/local_projects/spark/ wiki_extracted/AA/wiki_00', encoding='utf-8') as f:\n",
    "    data_json = []\n",
    "    for line in f:\n",
    "        data_json.append(json.loads(line))\n",
    "pd_df = pd.DataFrame(data_json)\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>7736826</td>\n",
       "      <td>Lebe lauter\\n\\nLebe lauter () is the third stu...</td>\n",
       "      <td>Cyprus at the 1988 Winter Olympics</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7716931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "      <td>4577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>7736826</td>\n",
       "      <td>Lebe lauter\\n\\nLebe lauter () is the third stu...</td>\n",
       "      <td>Cyprus at the 1988 Winter Olympics</td>\n",
       "      <td>https://en.wikipedia.org/wiki?curid=7716931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- text: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n\n+-------+--------------------+--------------------+--------------------+\n|     id|                text|               title|                 url|\n+-------+--------------------+--------------------+--------------------+\n|7697612|Stockton Airport\n...|    Stockton Airport|https://en.wikipe...|\n|7697675|Lobo (wrestler)\n\n...|     Lobo (wrestler)|https://en.wikipe...|\n|7697715|Anti-submarine mi...|Anti-submarine mi...|https://en.wikipe...|\n+-------+--------------------+--------------------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame(pd_df)\n",
    "data.printSchema()\n",
    "data.sample(False, 0.27).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol=\"filtered\")   \n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\",numFeatures=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering by K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, kmeans])\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test and print interested columns of different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n|     id|               title|prediction|\n+-------+--------------------+----------+\n|7697605|  Konica Minolta Cup|         0|\n|7697611|   Archer (typeface)|         0|\n|7697612|    Stockton Airport|         0|\n|7697626|        Ricky Minard|         0|\n|7697641|      Alexander Peya|         0|\n|7697655|  Swiss chalet style|         0|\n|7697664|European Federati...|         0|\n|7697671|The Best Is Yet t...|         0|\n|7697675|     Lobo (wrestler)|         0|\n|7697715|Anti-submarine mi...|         0|\n+-------+--------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+\n|     id|               title|prediction|\n+-------+--------------------+----------+\n|7698038|      Radamel Falcao|         1|\n|7698053| Panjshir offensives|         1|\n|7698941|Istanbul High School|         1|\n|7699151|The Market for Li...|         1|\n|7699200|     Parchís (group)|         1|\n|7700918|            Manikata|         1|\n|7701000|2007 Major League...|         1|\n|7701470|World War II pers...|         1|\n|7701711|      Luck by Chance|         1|\n|7702313|       Yoga Vasistha|         1|\n+-------+--------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|     154|\n|    4423|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(data)\n",
    "predictions.registerTempTable(\"predictions\")\n",
    "spark.sql(\"SELECT id, title, prediction FROM predictions WHERE prediction = '0'\").show(10)\n",
    "spark.sql(\"SELECT id, title, prediction FROM predictions WHERE prediction = '1'\").show(10)\n",
    "spark.sql(\"SELECT count(*) FROM predictions GROUP BY prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 Part 1: Fist Dataset with Logistic Regression and NaiveBayes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required labraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.clustering import BisectingKMeans, KMeans, GaussianMixture\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, NGram, IDF, StopWordsRemover\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dataset1_classification\") \\\n",
    "    .config(\"spark.som.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Vectorize the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 3...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 3...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/vibrioh/local_projects/spark/datasets/ta/set1/pima-indians-diabetes.data', encoding='utf-8') as f:\n",
    "    col1 = []\n",
    "    col2 = []\n",
    "    for line in f:\n",
    "        parts = line.split(',')\n",
    "        label = float(parts[len(parts)-1])\n",
    "        features = Vectors.dense([float(parts[x]) for x in range(0,len(parts)-1)])\n",
    "        col1.append(label)\n",
    "        col2.append(features)\n",
    "    dict = {\"label\": col1, \"features\": col2}\n",
    "pd_d1 = pd.DataFrame(dict)\n",
    "pd_d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- features: vector (nullable = true)\n |-- label: double (nullable = true)\n\n+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[8.0,183.0,64.0,0...|  1.0|\n|[1.0,89.0,66.0,23...|  0.0|\n|[10.0,115.0,0.0,0...|  0.0|\n+--------------------+-----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data1 = spark.createDataFrame(pd_d1)\n",
    "data1.printSchema()\n",
    "data1.sample(False, 0.27).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = data1.randomSplit([0.8, 0.2], seed=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression and Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer and set its parameters\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "nbModel = nb.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classifier predictions\n+--------------------+-----+--------------------+-----------+----------+\n|            features|label|       rawPrediction|probability|prediction|\n+--------------------+-----+--------------------+-----------+----------+\n|[0.0,118.0,84.0,4...|  1.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[0.0,119.0,64.0,1...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[0.0,125.0,96.0,0...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[0.0,146.0,82.0,0...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,0.0,74.0,20....|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,89.0,66.0,23...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,95.0,66.0,13...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,101.0,50.0,1...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,109.0,56.0,2...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,115.0,70.0,3...|  1.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,126.0,56.0,2...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[1.0,163.0,72.0,0...|  1.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[2.0,85.0,65.0,0....|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[2.0,100.0,66.0,2...|  1.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[2.0,110.0,74.0,2...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[3.0,83.0,58.0,31...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[3.0,88.0,58.0,11...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[3.0,128.0,78.0,0...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[3.0,158.0,76.0,3...|  1.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n|[4.0,103.0,60.0,3...|  0.0|[0.61903920840622...|[0.65,0.35]|       0.0|\n+--------------------+-----+--------------------+-----------+----------+\nonly showing top 20 rows\n\nNaive Bayes classifier predictions\n+--------------------+-----+--------------------+--------------------+----------+\n|            features|label|       rawPrediction|         probability|prediction|\n+--------------------+-----+--------------------+--------------------+----------+\n|[0.0,118.0,84.0,4...|  1.0|[-940.61590258077...|[0.00222035512724...|       1.0|\n|[0.0,119.0,64.0,1...|  0.0|[-570.93042316226...|[0.34024409829781...|       1.0|\n|[0.0,125.0,96.0,0...|  0.0|[-398.51412035553...|[0.99984493731386...|       0.0|\n|[0.0,146.0,82.0,0...|  0.0|[-507.15808445073...|[0.99944541772140...|       0.0|\n|[1.0,0.0,74.0,20....|  0.0|[-333.42573140269...|[0.99999659118712...|       0.0|\n|[1.0,89.0,66.0,23...|  0.0|[-537.77736461550...|[0.72794260819999...|       0.0|\n|[1.0,95.0,66.0,13...|  0.0|[-419.94009000027...|[0.98632919581248...|       0.0|\n|[1.0,101.0,50.0,1...|  0.0|[-418.12769782653...|[0.92458101473041...|       0.0|\n|[1.0,109.0,56.0,2...|  0.0|[-604.05216987817...|[0.00475046122727...|       1.0|\n|[1.0,115.0,70.0,3...|  1.0|[-639.76679384547...|[0.82243219784355...|       0.0|\n|[1.0,126.0,56.0,2...|  0.0|[-675.05572884516...|[0.00121490185273...|       1.0|\n|[1.0,163.0,72.0,0...|  1.0|[-481.24197102653...|[0.99041485445669...|       0.0|\n|[2.0,85.0,65.0,0....|  0.0|[-373.38413562769...|[0.99933714490051...|       0.0|\n|[2.0,100.0,66.0,2...|  1.0|[-572.78674440437...|[0.66969877398212...|       0.0|\n|[2.0,110.0,74.0,2...|  0.0|[-671.37589458991...|[0.29372603707675...|       1.0|\n|[3.0,83.0,58.0,31...|  0.0|[-457.68905156759...|[0.99967549740400...|       0.0|\n|[3.0,88.0,58.0,11...|  0.0|[-432.71723920739...|[0.83926761944996...|       0.0|\n|[3.0,128.0,78.0,0...|  0.0|[-464.07127210943...|[0.99897195376000...|       0.0|\n|[3.0,158.0,76.0,3...|  1.0|[-939.34332498455...|[1.87311685420356...|       1.0|\n|[4.0,103.0,60.0,3...|  0.0|[-761.19283980910...|[2.13987700098319...|       1.0|\n+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "lrPredictions = lrModel.transform(test)\n",
    "print(\"Logistic Regression classifier predictions\")\n",
    "lrPredictions.show()\n",
    "nbPredictions = nbModel.transform(test)\n",
    "print(\"Naive Bayes classifier predictions\")\n",
    "nbPredictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalue the modles with AUC and overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n \t Area under ROC curve (AUC): 0.5 \n\t Accuracy:  0.6554054054054054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: \n \t Area under ROC curve (AUC): 0.29694764503739646 \n\t Accuracy:  0.7027027027027027\n"
     ]
    }
   ],
   "source": [
    "evaluator1 = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "evaluator2 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print (\"Logistic Regression: \\n\", \"\\t Area under ROC curve (AUC):\", evaluator1.evaluate(lrPredictions), \"\\n\\t Accuracy: \", evaluator2.evaluate(lrPredictions))\n",
    "print (\"Naive Bayes: \\n\", \"\\t Area under ROC curve (AUC):\", evaluator1.evaluate(nbPredictions), \"\\n\\t Accuracy: \", evaluator2.evaluate(nbPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple comparison on the two modles's performance\n",
    "\n",
    "### Overall Accuracy:\n",
    "\n",
    "- Naive Bayes classifier carried out a higher overall accuracy (0.70 vs 0.66)\n",
    "- Both modle can yield predictions accuracy than 0.5 of random\n",
    "- So Naive Bayes made better classification over Logistic Regression on this training/test set\n",
    "\n",
    "### Area under ROC curve (AUC):\n",
    "\n",
    "- Logistic Regression classifier carried out a higher AUC (0.5 vs 0.3)\n",
    "- Logistic Regression classifier has higher discriminative power over class distribution\n",
    "\n",
    "### In summary, on the selected training/test set, Naive Bayes classifier has the better result. However, the Logistic Regression classifier may be more stable on other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 Part 2: Second Dataset with Logistic Regression and NaiveBayes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required labraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.clustering import BisectingKMeans, KMeans, GaussianMixture\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, NGram, IDF, StopWordsRemover\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dataset2_classification\") \\\n",
    "    .config(\"spark.som.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Vectorize the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 22.08, 11.46, 2.0, 4.0, 4.0, 1.585, 0.0,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 22.67, 7.0, 2.0, 8.0, 4.0, 0.165, 0.0, 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 29.58, 1.75, 1.0, 4.0, 4.0, 1.25, 0.0, 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 21.67, 11.5, 1.0, 5.0, 3.0, 0.0, 1.0, 1....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0, 20.17, 8.17, 2.0, 6.0, 4.0, 1.96, 1.0, 1...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 22.08, 11.46, 2.0, 4.0, 4.0, 1.585, 0.0,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 22.67, 7.0, 2.0, 8.0, 4.0, 0.165, 0.0, 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 29.58, 1.75, 1.0, 4.0, 4.0, 1.25, 0.0, 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 21.67, 11.5, 1.0, 5.0, 3.0, 0.0, 1.0, 1....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0, 20.17, 8.17, 2.0, 6.0, 4.0, 1.96, 1.0, 1...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/vibrioh/local_projects/spark/datasets/ta/set2/australian.dat', encoding='utf-8') as f:\n",
    "    col1 = []\n",
    "    col2 = []\n",
    "    for line in f:\n",
    "        parts = line.split(' ')\n",
    "        label = float(parts[len(parts)-1])\n",
    "        features = Vectors.dense([float(parts[x]) for x in range(0,len(parts)-1)])\n",
    "        col1.append(label)\n",
    "        col2.append(features)\n",
    "    dict = {\"label\": col1, \"features\": col2}\n",
    "pd_d2 = pd.DataFrame(dict)\n",
    "pd_d2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- features: vector (nullable = true)\n |-- label: double (nullable = true)\n\n+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[0.0,21.67,11.5,1...|  1.0|\n|[1.0,20.17,8.17,2...|  1.0|\n|[0.0,15.83,0.585,...|  1.0|\n+--------------------+-----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data2 = spark.createDataFrame(pd_d2)\n",
    "data2.printSchema()\n",
    "data2.sample(False, 0.27).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = data2.randomSplit([0.8, 0.2], seed=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Logistic Regression and Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer and set its parameters\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "nbModel = nb.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classifier predictions\n+--------------------+-----+--------------------+--------------------+----------+\n|            features|label|       rawPrediction|         probability|prediction|\n+--------------------+-----+--------------------+--------------------+----------+\n|[0.0,20.42,10.5,1...|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[0.0,20.75,10.25,...|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[0.0,20.75,10.335...|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[0.0,22.67,7.0,2....|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[0.0,24.75,12.5,2...|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[0.0,30.67,12.0,2...|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[0.0,32.17,1.46,2...|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[0.0,35.75,0.915,...|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[0.0,38.92,1.665,...|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[0.0,39.08,4.0,2....|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[0.0,47.0,13.0,2....|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[0.0,55.75,7.08,2...|  0.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[1.0,16.17,0.04,2...|  1.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[1.0,19.0,0.0,1.0...|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[1.0,20.0,1.25,1....|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[1.0,21.5,11.5,2....|  0.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[1.0,22.0,0.79,2....|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[1.0,22.67,1.585,...|  1.0|[-0.1767811960708...|[0.45591944020243...|       1.0|\n|[1.0,23.08,0.0,2....|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n|[1.0,23.75,0.415,...|  0.0|[0.53467906597270...|[0.63057376704516...|       0.0|\n+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 20 rows\n\nNaive Bayes classifier predictions\n+--------------------+-----+--------------------+--------------------+----------+\n|            features|label|       rawPrediction|         probability|prediction|\n+--------------------+-----+--------------------+--------------------+----------+\n|[0.0,20.42,10.5,1...|  0.0|[-374.71894703985...|[1.0,7.1691492545...|       0.0|\n|[0.0,20.75,10.25,...|  1.0|[-272.61645936976...|[1.0,7.9512154983...|       0.0|\n|[0.0,20.75,10.335...|  1.0|[-358.18373217373...|[1.0,2.7828549813...|       0.0|\n|[0.0,22.67,7.0,2....|  0.0|[-296.52550073657...|[1.0,6.1034404981...|       0.0|\n|[0.0,24.75,12.5,2...|  1.0|[-882.80205063407...|[9.16938606433626...|       1.0|\n|[0.0,30.67,12.0,2...|  1.0|[-437.66967570185...|[1.0,2.3001494432...|       0.0|\n|[0.0,32.17,1.46,2...|  1.0|[-2172.3166551623...|           [0.0,1.0]|       1.0|\n|[0.0,35.75,0.915,...|  1.0|[-1569.0382419969...|           [0.0,1.0]|       1.0|\n|[0.0,38.92,1.665,...|  0.0|[-516.68712024168...|[2.85619336654972...|       1.0|\n|[0.0,39.08,4.0,2....|  0.0|[-590.51320509544...|           [1.0,0.0]|       0.0|\n|[0.0,47.0,13.0,2....|  1.0|[-361.69906371960...|[1.0,4.2718077713...|       0.0|\n|[0.0,55.75,7.08,2...|  0.0|[-465.09686661564...|[1.0,3.1852541001...|       0.0|\n|[1.0,16.17,0.04,2...|  1.0|[-127.34207448934...|[1.0,1.2049030087...|       0.0|\n|[1.0,19.0,0.0,1.0...|  0.0|[-154.55466029514...|[1.0,2.9832451582...|       0.0|\n|[1.0,20.0,1.25,1....|  0.0|[-232.95891654432...|[1.0,1.7522842178...|       0.0|\n|[1.0,21.5,11.5,2....|  0.0|[-328.39292616749...|[1.0,1.4690448845...|       0.0|\n|[1.0,22.0,0.79,2....|  0.0|[-733.32230070910...|[1.0,1.8294086996...|       0.0|\n|[1.0,22.67,1.585,...|  1.0|[-285.51305623302...|[1.0,4.5883044328...|       0.0|\n|[1.0,23.08,0.0,2....|  0.0|[-208.73577776562...|[1.0,6.0553106679...|       0.0|\n|[1.0,23.75,0.415,...|  0.0|[-268.99747875558...|[1.0,3.0571078699...|       0.0|\n+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "lrPredictions = lrModel.transform(test)\n",
    "print(\"Logistic Regression classifier predictions\")\n",
    "lrPredictions.show()\n",
    "nbPredictions = nbModel.transform(test)\n",
    "print(\"Naive Bayes classifier predictions\")\n",
    "nbPredictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalue the modles with AUC and overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n \t Area under ROC curve (AUC): 0.8758116883116883 \n\t Accuracy:  0.8646616541353384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: \n \t Area under ROC curve (AUC): 0.3773191094619667 \n\t Accuracy:  0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "evaluator1 = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "evaluator2 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print (\"Logistic Regression: \\n\", \"\\t Area under ROC curve (AUC):\", evaluator1.evaluate(lrPredictions), \"\\n\\t Accuracy: \", evaluator2.evaluate(lrPredictions))\n",
    "print (\"Naive Bayes: \\n\", \"\\t Area under ROC curve (AUC):\", evaluator1.evaluate(nbPredictions), \"\\n\\t Accuracy: \", evaluator2.evaluate(nbPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple comparison on the two modles's performance\n",
    "\n",
    "### Overall Accuracy:\n",
    "\n",
    "- Logistic Regression classifier carried out a higher overall accuracy (0.86 vs 0.63)\n",
    "- Both modle can yield predictions accuracy than 0.5 of random\n",
    "- So Logistic Regression made better classification over Naive Bayes on this training/test set\n",
    "\n",
    "### Area under ROC curve (AUC):\n",
    "\n",
    "- Logistic Regression classifier carried out a higher AUC (0.88 vs 0.38)\n",
    "- Logistic Regression classifier has higher discriminative power over class distribution\n",
    "\n",
    "### In summary, the Logistic Regression classifier has much better performance over the Naive Bayes, whatever on this perticular training/test set or potential future test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 Part 3: Wikipedia Dataset with Logistic Regression and NaiveBayes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required labraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.clustering import BisectingKMeans, KMeans, GaussianMixture\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, NGram, IDF, StopWordsRemover\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"wikipedia_classification\") \\\n",
    "    .config(\"spark.som.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the raw data and assigne lables with clustering results by K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- text: string (nullable = true)\n |-- assignedLable: integer (nullable = true)\n\n+-------+------------------+--------------------+-------------+\n|     id|             title|                text|assignedLable|\n+-------+------------------+--------------------+-------------+\n|7697605|Konica Minolta Cup|Konica Minolta Cu...|            0|\n|7697611| Archer (typeface)|Archer (typeface)...|            0|\n+-------+------------------+--------------------+-------------+\nonly showing top 2 rows\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|     154|\n|    4423|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "wikiDF = spark.sql(\"SELECT id, title, text, prediction as assignedLable FROM predictions\")\n",
    "#print schema\n",
    "wikiDF.printSchema()\n",
    "wikiDF.registerTempTable(\"wikiDF\")\n",
    "wikiDF.show(2)\n",
    "spark.sql(\"SELECT count(*) FROM wikiDF GROUP BY assignedLable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cast assigned lables to double and store to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- text: string (nullable = true)\n |-- assignedLable: integer (nullable = true)\n |-- label: double (nullable = true)\n\n+-------+--------------------+--------------------+-------------+-----+\n|     id|               title|                text|assignedLable|label|\n+-------+--------------------+--------------------+-------------+-----+\n|7697715|Anti-submarine mi...|Anti-submarine mi...|            0|  0.0|\n|7697725|Northern river shark|Northern river sh...|            0|  0.0|\n+-------+--------------------+--------------------+-------------+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "wikiDF = wikiDF.withColumn(\"label\", wikiDF.assignedLable.cast(\"double\"))\n",
    "wikiDF.printSchema()\n",
    "wikiDF.sample(False, 0.23).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Training (80%) and Test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total document count: 4577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-set count: 3645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set count: 932\n"
     ]
    }
   ],
   "source": [
    "training, test = wikiDF.randomSplit([0.8, 0.2], 2388)\n",
    "print (\"Total document count:\",wikiDF.count())\n",
    "print (\"Training-set count:\",training.count())\n",
    "print (\"Test-set count:\",test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config the pipeines for Logistic Regression and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n",
    "hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "lr = LogisticRegression().setRegParam(0.01).setThreshold(0.5)\n",
    "nb = NaiveBayes().setModelType(\"multinomial\").setSmoothing(1.0)\n",
    "lrPipeline=Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
    "nbPipeline=Pipeline(stages=[tokenizer, remover, hashingTF, idf, nb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train  Logistic Regression and Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lrPipeline.fit(training)\n",
    "nbModel = nbPipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression classifier predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n|     id|label|prediction|\n+-------+-----+----------+\n|7697757|  0.0|       0.0|\n|7697782|  0.0|       0.0|\n|7697786|  0.0|       0.0|\n|7697794|  0.0|       0.0|\n|7697805|  0.0|       0.0|\n+-------+-----+----------+\nonly showing top 5 rows\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n|     id|label|prediction|\n+-------+-----+----------+\n|7698941|  1.0|       1.0|\n|7701470|  1.0|       1.0|\n|7703762|  1.0|       1.0|\n|7705039|  1.0|       1.0|\n|7705856|  1.0|       1.0|\n+-------+-----+----------+\nonly showing top 5 rows\n\nNaive Bayes classifier predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n|     id|label|prediction|\n+-------+-----+----------+\n|7697757|  0.0|       0.0|\n|7697782|  0.0|       0.0|\n|7697786|  0.0|       0.0|\n|7697794|  0.0|       0.0|\n|7697808|  0.0|       0.0|\n+-------+-----+----------+\nonly showing top 5 rows\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+\n|     id|label|prediction|\n+-------+-----+----------+\n|7697805|  0.0|       1.0|\n|7698625|  0.0|       1.0|\n|7699045|  0.0|       1.0|\n|7699145|  0.0|       1.0|\n|7699710|  0.0|       1.0|\n+-------+-----+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "lrPredictions = lrModel.transform(test)\n",
    "lrPredictions.registerTempTable(\"lrPredictions\")\n",
    "print(\"Logistic Regression classifier predictions\")\n",
    "spark.sql(\"SELECT id, label, prediction FROM lrPredictions WHERE prediction = '0'\").show(5)\n",
    "spark.sql(\"SELECT id, label, prediction FROM lrPredictions WHERE prediction = '1'\").show(5)\n",
    "nbPredictions = nbModel.transform(test)\n",
    "nbPredictions.registerTempTable(\"nbPredictions\")\n",
    "print(\"Naive Bayes classifier predictions\")\n",
    "spark.sql(\"SELECT id, label, prediction FROM nbPredictions WHERE prediction = '0'\").show(5)\n",
    "spark.sql(\"SELECT id, label, prediction FROM nbPredictions WHERE prediction = '1'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models with Area under ROC curve (AUC) and overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n \t Area under ROC curve (AUC): 0.9990286117979512 \n\t Accuracy:  0.9924892703862661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: \n \t Area under ROC curve (AUC): 0.00029436006122689424 \n\t Accuracy:  0.8594420600858369\n"
     ]
    }
   ],
   "source": [
    "evaluator1 = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "evaluator2 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print (\"Logistic Regression: \\n\", \"\\t Area under ROC curve (AUC):\", evaluator1.evaluate(lrPredictions), \"\\n\\t Accuracy: \", evaluator2.evaluate(lrPredictions))\n",
    "print (\"Naive Bayes: \\n\", \"\\t Area under ROC curve (AUC):\", evaluator1.evaluate(nbPredictions), \"\\n\\t Accuracy: \", evaluator2.evaluate(nbPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple comparison on the two modles's performance\n",
    "\n",
    "### Overall Accuracy:\n",
    "\n",
    "- lables are assigned by K-MEANS clustering, so overall accuracy was very high\n",
    "- Logistic Regression classifier carried out a higher overall accuracy (0.99 vs 0.86)\n",
    "- Both modle can yield predictions accuracy than 0.5 of random\n",
    "- So Logistic Regression made better classification over Naive Bayes on this training/test set\n",
    "\n",
    "### Area under ROC curve (AUC):\n",
    "\n",
    "- Logistic Regression classifier carried out a much higher AUC (0.85 vs 0.00)\n",
    "- Logistic Regression classifier has higher discriminative power over class distribution\n",
    "\n",
    "### In summary, the Logistic Regression classifier has much better performance over the Naive Bayes, whatever on this perticular training/test set or potential future test datasets.\n",
    "\n",
    "### **In our raw data showed at the beginning, one class had much more data points (4423) than the other class (154). This bias significantly affected Naive Bayes classifier -- it had very hight error costs (false positive and false negative cost), but not the Logistic Regression classifier. So in our experiment, at least we can conclude that under this kind of bias, we should avoid using Naive Bayes classifier.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}