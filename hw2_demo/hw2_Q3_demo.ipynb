{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create/import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (1.0, \"happy\"),\n",
    "    (1.0, \"happy,happy\"),\n",
    "    (0.0, \"happy,sad\"),\n",
    "    (0.0, \"sad\"),\n",
    "    (0.0, \"sad,happy\"),\n",
    "    (1.0, \"happy,happy\")\n",
    "], [\"label\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "df = spark.read.json('/Users/gongqian/Desktop/spark_test/Q3/russian/train_small.json')\n",
    "sentenceData = df.select(\"sentiment\",\"text\").toDF(\"label\",\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|label|text       |\n",
      "+-----+-----------+\n",
      "|1.0  |happy      |\n",
      "|1.0  |happy,happy|\n",
      "|0.0  |happy,sad  |\n",
      "|0.0  |sad        |\n",
      "|0.0  |sad,happy  |\n",
      "|1.0  |happy,happy|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic={happy:0,sad:1}\n",
    "1 0:1\n",
    "1 0:2\n",
    "0 0:1,1:1\n",
    "0 1:1\n",
    "0 1:1,0:1\n",
    "1 0:2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+\n",
      "|label|text       |words        |\n",
      "+-----+-----------+-------------+\n",
      "|1.0  |happy      |[happy]      |\n",
      "|1.0  |happy,happy|[happy,happy]|\n",
      "|0.0  |happy,sad  |[happy,sad]  |\n",
      "|0.0  |sad        |[sad]        |\n",
      "|0.0  |sad,happy  |[sad,happy]  |\n",
      "|1.0  |happy,happy|[happy,happy]|\n",
      "+-----+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsData.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=2)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+-------------+\n",
      "|label|text       |words        |rawFeatures  |\n",
      "+-----+-----------+-------------+-------------+\n",
      "|1.0  |happy      |[happy]      |(2,[1],[1.0])|\n",
      "|1.0  |happy,happy|[happy,happy]|(2,[1],[1.0])|\n",
      "|0.0  |happy,sad  |[happy,sad]  |(2,[0],[1.0])|\n",
      "|0.0  |sad        |[sad]        |(2,[0],[1.0])|\n",
      "|0.0  |sad,happy  |[sad,happy]  |(2,[0],[1.0])|\n",
      "|1.0  |happy,happy|[happy,happy]|(2,[1],[1.0])|\n",
      "+-----+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------+\n",
      "|label|features                    |\n",
      "+-----+----------------------------+\n",
      "|1.0  |(2,[1],[0.5596157879354227])|\n",
      "|1.0  |(2,[1],[0.5596157879354227])|\n",
      "|0.0  |(2,[0],[0.5596157879354227])|\n",
      "|0.0  |(2,[0],[0.5596157879354227])|\n",
      "|0.0  |(2,[0],[0.5596157879354227])|\n",
      "|1.0  |(2,[1],[0.5596157879354227])|\n",
      "+-----+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = rescaledData.select(\"label\", \"features\").randomSplit([0.8, 0.2], 1234)\n",
    "train = splits[1]\n",
    "test = splits[0]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(2,[0],[0.5596157...|\n",
      "|  1.0|(2,[1],[0.5596157...|\n",
      "|  1.0|(2,[1],[0.5596157...|\n",
      "|  0.0|(2,[0],[0.5596157...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(2,[0],[0.5596157...|[-0.9094572187149...|[0.60355537694346...|       0.0|\n",
      "|  1.0|(2,[1],[0.5596157...|[-1.3297586477855...|[0.39644462305653...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "+-----+----------------------------+-----------------------------------------+----------------------------------------+----------+\n",
      "|label|features                    |rawPrediction                            |probability                             |prediction|\n",
      "+-----+----------------------------+-----------------------------------------+----------------------------------------+----------+\n",
      "|0.0  |(2,[0],[0.5596157879354227])|[-0.9094572187149538,-1.3297586477855063]|[0.6035553769434613,0.39644462305653877]|0.0       |\n",
      "|1.0  |(2,[1],[0.5596157879354227])|[-1.3297586477855063,-0.9094572187149538]|[0.39644462305653877,0.6035553769434613]|1.0       |\n",
      "+-----+----------------------------+-----------------------------------------+----------------------------------------+----------+\n",
      "\n",
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "predictions.show(30,False)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
