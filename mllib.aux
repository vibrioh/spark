\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\HyPL@Entry{1<</S/D>>}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Question 1 Part 1: Building an Explicit Movie Recommendation System with Spark MLlib}{4}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{question-1-part-1-building-an-explicit-movie-recommendation-system-with-spark-mllib}{{1}{4}{Question 1 Part 1: Building an Explicit Movie Recommendation System with Spark MLlib}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Import required libraries}{4}{section.1.1}}
\newlabel{import-required-libraries}{{1.1}{4}{Import required libraries}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Download/Unzip the MovieLens 1M dataset from http://grouplens.org/datasets/movielens}{4}{section.1.2}}
\newlabel{downloadunzip-the-movielens-1m-dataset-from-httpgrouplens.orgdatasetsmovielens}{{1.2}{4}{Download/Unzip the MovieLens 1M dataset from http://grouplens.org/datasets/movielens}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Read and Convert ratings data to a DataFrame}{4}{section.1.3}}
\newlabel{read-and-convert-ratings-data-to-a-dataframe}{{1.3}{4}{Read and Convert ratings data to a DataFrame}{section.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Show the number of ratings in the dataset}{5}{section.1.4}}
\newlabel{show-the-number-of-ratings-in-the-dataset}{{1.4}{5}{Show the number of ratings in the dataset}{section.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Show a sample of the Ratings DataFrame}{5}{section.1.5}}
\newlabel{show-a-sample-of-the-ratings-dataframe}{{1.5}{5}{Show a sample of the Ratings DataFrame}{section.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Show sample number of ratings per user}{5}{section.1.6}}
\newlabel{show-sample-number-of-ratings-per-user}{{1.6}{5}{Show sample number of ratings per user}{section.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Show the number of users in the dataset}{6}{section.1.7}}
\newlabel{show-the-number-of-users-in-the-dataset}{{1.7}{6}{Show the number of users in the dataset}{section.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Split Ratings data into Training (80\%) and Test (20\%) datasets}{6}{section.1.8}}
\newlabel{split-ratings-data-into-training-80-and-test-20-datasets}{{1.8}{6}{Split Ratings data into Training (80\%) and Test (20\%) datasets}{section.1.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Show resulting Ratings dataset counts}{6}{section.1.9}}
\newlabel{show-resulting-ratings-dataset-counts}{{1.9}{6}{Show resulting Ratings dataset counts}{section.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Build the recommendation model on the training data using ALS-explicit}{6}{section.1.10}}
\newlabel{build-the-recommendation-model-on-the-training-data-using-als-explicit}{{1.10}{6}{Build the recommendation model on the training data using ALS-explicit}{section.1.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Run the model against the Test data and show a sample of the predictions}{6}{section.1.11}}
\newlabel{run-the-model-against-the-test-data-and-show-a-sample-of-the-predictions}{{1.11}{6}{Run the model against the Test data and show a sample of the predictions}{section.1.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.12}Evaluate the model by computing the RMSE on the test data}{7}{section.1.12}}
\newlabel{evaluate-the-model-by-computing-the-rmse-on-the-test-data}{{1.12}{7}{Evaluate the model by computing the RMSE on the test data}{section.1.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.13}Show that a smaller value of rmse is better}{7}{section.1.13}}
\newlabel{show-that-a-smaller-value-of-rmse-is-better}{{1.13}{7}{Show that a smaller value of rmse is better}{section.1.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.14}Make movie recommendations}{7}{section.1.14}}
\newlabel{make-movie-recommendations}{{1.14}{7}{Make movie recommendations}{section.1.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.15}Show sample recommendations per user}{7}{section.1.15}}
\newlabel{show-sample-recommendations-per-user}{{1.15}{7}{Show sample recommendations per user}{section.1.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.16}Show sample recommendations per user}{8}{section.1.16}}
\newlabel{show-sample-recommendations-per-user}{{1.16}{8}{Show sample recommendations per user}{section.1.16}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Question 1 Part 2: Building an Implicit Music Recommendation System with Spark MLlib}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{question-1-part-2-building-an-implicit-music-recommendation-system-with-spark-mllib}{{2}{9}{Question 1 Part 2: Building an Implicit Music Recommendation System with Spark MLlib}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Import required libraries}{9}{section.2.1}}
\newlabel{import-required-libraries}{{2.1}{9}{Import required libraries}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Download/Unzip Audioscrobbler dataset from http://www.iro.umontreal.ca/\textasciitilde {}lisa/datasets/profiledata\_06-May-2005.tar.gz}{9}{section.2.2}}
\newlabel{downloadunzip-audioscrobbler-dataset-from-httpwww.iro.umontreal.calisadatasetsprofiledata_06-may-2005.tar.gz}{{2.2}{9}{Download/Unzip Audioscrobbler dataset from http://www.iro.umontreal.ca/\textasciitilde {}lisa/datasets/profiledata\_06-May-2005.tar.gz}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Read and Convert ratings data to a DataFrame}{9}{section.2.3}}
\newlabel{read-and-convert-ratings-data-to-a-dataframe}{{2.3}{9}{Read and Convert ratings data to a DataFrame}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Show the number of userArtist in the dataset}{10}{section.2.4}}
\newlabel{show-the-number-of-userartist-in-the-dataset}{{2.4}{10}{Show the number of userArtist in the dataset}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Show a sample of the userArtist DataFrame}{10}{section.2.5}}
\newlabel{show-a-sample-of-the-userartist-dataframe}{{2.5}{10}{Show a sample of the userArtist DataFrame}{section.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Split userArtist data into Training (80\%) and Test (20\%) datasets}{10}{section.2.6}}
\newlabel{split-userartist-data-into-training-80-and-test-20-datasets}{{2.6}{10}{Split userArtist data into Training (80\%) and Test (20\%) datasets}{section.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Show resulting userArtist dataset counts}{10}{section.2.7}}
\newlabel{show-resulting-userartist-dataset-counts}{{2.7}{10}{Show resulting userArtist dataset counts}{section.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Build the recommendation model on the training data using ALS-implicit}{10}{section.2.8}}
\newlabel{build-the-recommendation-model-on-the-training-data-using-als-implicit}{{2.8}{10}{Build the recommendation model on the training data using ALS-implicit}{section.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Save and load model}{11}{section.2.9}}
\newlabel{save-and-load-model}{{2.9}{11}{Save and load model}{section.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Run the model against the Test data and show a sample of the predictions}{11}{section.2.10}}
\newlabel{run-the-model-against-the-test-data-and-show-a-sample-of-the-predictions}{{2.10}{11}{Run the model against the Test data and show a sample of the predictions}{section.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}Show recommendations high and low}{11}{section.2.11}}
\newlabel{show-recommendations-high-and-low}{{2.11}{11}{Show recommendations high and low}{section.2.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Question 2 Part 1: Clustering on News Articles}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{question-2-part-1-clustering-on-news-articles}{{3}{13}{Question 2 Part 1: Clustering on News Articles}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Download news articles from: https://www.kaggle.com/asad1m9a9h6mood/news-articles}{13}{section.3.1}}
\newlabel{download-news-articles-from-httpswww.kaggle.comasad1m9a9h6moodnews-articles}{{3.1}{13}{Download news articles from: https://www.kaggle.com/asad1m9a9h6mood/news-articles}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}This Dataset is scraped from https://www.thenews.com.pk website. It has news articles from 2015 till date related to business and sports. It Contains the Heading of the particular Article, Its content and its date. The content also contains the place from where the statement or Article was published.}{13}{subsection.3.1.1}}
\newlabel{this-dataset-is-scraped-from-httpswww.thenews.com.pk-website.-it-has-news-articles-from-2015-till-date-related-to-business-and-sports.-it-contains-the-heading-of-the-particular-article-its-content-and-its-date.-the-content-also-contains-the-place-from-where-the-statement-or-article-was-published.}{{3.1.1}{13}{This Dataset is scraped from https://www.thenews.com.pk website. It has news articles from 2015 till date related to business and sports. It Contains the Heading of the particular Article, Its content and its date. The content also contains the place from where the statement or Article was published}{subsection.3.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Import required libraries}{13}{section.3.2}}
\newlabel{import-required-libraries}{{3.2}{13}{Import required libraries}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Explore the dataset in Pandas}{13}{section.3.3}}
\newlabel{explore-the-dataset-in-pandas}{{3.3}{13}{Explore the dataset in Pandas}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Split data into Training (80\%) and Test (20\%) datasets}{16}{section.3.4}}
\newlabel{split-data-into-training-80-and-test-20-datasets}{{3.4}{16}{Split data into Training (80\%) and Test (20\%) datasets}{section.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Configure an ML pipeline}{16}{section.3.5}}
\newlabel{configure-an-ml-pipeline}{{3.5}{16}{Configure an ML pipeline}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Clustering by K-MEANS}{16}{section.3.6}}
\newlabel{clustering-by-k-means}{{3.6}{16}{Clustering by K-MEANS}{section.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Make predictions on test and print interested columns of different clusters}{17}{section.3.7}}
\newlabel{make-predictions-on-test-and-print-interested-columns-of-different-clusters}{{3.7}{17}{Make predictions on test and print interested columns of different clusters}{section.3.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Question 2 Part 2: Clustering on Wikipedia articles}{18}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{question-2-part-2-clustering-on-wikipedia-articles}{{4}{18}{Question 2 Part 2: Clustering on Wikipedia articles}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Import required libraries}{18}{section.4.1}}
\newlabel{import-required-libraries}{{4.1}{18}{Import required libraries}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Download/Unzip https://dumps.wikimedia.org/enwiki/20170920/enwiki-20170920-pages-articles14.xml-p7697599p7744799.bz2}{18}{section.4.2}}
\newlabel{downloadunzip-httpsdumps.wikimedia.orgenwiki20170920enwiki-20170920-pages-articles14.xml-p7697599p7744799.bz2}{{4.2}{18}{Download/Unzip https://dumps.wikimedia.org/enwiki/20170920/enwiki-20170920-pages-articles14.xml-p7697599p7744799.bz2}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Parse xml to json using WikiExtractorw (https://github.com/attardi/wikiextractor)}{18}{section.4.3}}
\newlabel{parse-xml-to-json-using-wikiextractorw-httpsgithub.comattardiwikiextractor}{{4.3}{18}{Parse xml to json using WikiExtractorw (https://github.com/attardi/wikiextractor)}{section.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Explore the dataset in Pandas}{18}{section.4.4}}
\newlabel{explore-the-dataset-in-pandas}{{4.4}{18}{Explore the dataset in Pandas}{section.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Split data into Training (80\%) and Test (20\%) datasets}{21}{section.4.5}}
\newlabel{split-data-into-training-80-and-test-20-datasets}{{4.5}{21}{Split data into Training (80\%) and Test (20\%) datasets}{section.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Configure an ML pipeline}{21}{section.4.6}}
\newlabel{configure-an-ml-pipeline}{{4.6}{21}{Configure an ML pipeline}{section.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Clustering by K-MEANS}{22}{section.4.7}}
\newlabel{clustering-by-k-means}{{4.7}{22}{Clustering by K-MEANS}{section.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Make predictions on test and print interested columns of different clusters}{22}{section.4.8}}
\newlabel{make-predictions-on-test-and-print-interested-columns-of-different-clusters}{{4.8}{22}{Make predictions on test and print interested columns of different clusters}{section.4.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Question 3 Part 1: Fist Dataset with Logistic Regression and NaiveBayes classification}{24}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{question-3-part-1-fist-dataset-with-logistic-regression-and-naivebayes-classification}{{5}{24}{Question 3 Part 1: Fist Dataset with Logistic Regression and NaiveBayes classification}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Import required labraries}{24}{section.5.1}}
\newlabel{import-required-labraries}{{5.1}{24}{Import required labraries}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Read and Vectorize the raw data}{24}{section.5.2}}
\newlabel{read-and-vectorize-the-raw-data}{{5.2}{24}{Read and Vectorize the raw data}{section.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Split data into Training (80\%) and Test (20\%) datasets}{26}{section.5.3}}
\newlabel{split-data-into-training-80-and-test-20-datasets}{{5.3}{26}{Split data into Training (80\%) and Test (20\%) datasets}{section.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Train Logistic Regression and Naive Bayes models}{26}{section.5.4}}
\newlabel{train-logistic-regression-and-naive-bayes-models}{{5.4}{26}{Train Logistic Regression and Naive Bayes models}{section.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Display the predictions}{26}{section.5.5}}
\newlabel{display-the-predictions}{{5.5}{26}{Display the predictions}{section.5.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Evalue the modles with AUC and overall Accuracy}{28}{section.5.6}}
\newlabel{evalue-the-modles-with-auc-and-overall-accuracy}{{5.6}{28}{Evalue the modles with AUC and overall Accuracy}{section.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}A simple comparison on the two modles's performance}{28}{section.5.7}}
\newlabel{a-simple-comparison-on-the-two-modless-performance}{{5.7}{28}{A simple comparison on the two modles's performance}{section.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Overall Accuracy:}{28}{subsection.5.7.1}}
\newlabel{overall-accuracy}{{5.7.1}{28}{Overall Accuracy:}{subsection.5.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Area under ROC curve (AUC):}{28}{subsection.5.7.2}}
\newlabel{area-under-roc-curve-auc}{{5.7.2}{28}{Area under ROC curve (AUC):}{subsection.5.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}In summary, on the selected training/test set, Naive Bayes classifier has the better result. However, the Logistic Regression classifier may be more stable on other datasets.}{28}{subsection.5.7.3}}
\newlabel{in-summary-on-the-selected-trainingtest-set-naive-bayes-classifier-has-the-better-result.-however-the-logistic-regression-classifier-may-be-more-stable-on-other-datasets.}{{5.7.3}{28}{In summary, on the selected training/test set, Naive Bayes classifier has the better result. However, the Logistic Regression classifier may be more stable on other datasets}{subsection.5.7.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Question 3 Part 2: Second Dataset with Logistic Regression and NaiveBayes classification}{29}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{question-3-part-2-second-dataset-with-logistic-regression-and-naivebayes-classification}{{6}{29}{Question 3 Part 2: Second Dataset with Logistic Regression and NaiveBayes classification}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Import required labraries}{29}{section.6.1}}
\newlabel{import-required-labraries}{{6.1}{29}{Import required labraries}{section.6.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Read and Vectorize the raw data}{29}{section.6.2}}
\newlabel{read-and-vectorize-the-raw-data}{{6.2}{29}{Read and Vectorize the raw data}{section.6.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Split data into Training (80\%) and Test (20\%) datasets}{31}{section.6.3}}
\newlabel{split-data-into-training-80-and-test-20-datasets}{{6.3}{31}{Split data into Training (80\%) and Test (20\%) datasets}{section.6.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Train Logistic Regression and Naive Bayes models}{31}{section.6.4}}
\newlabel{train-logistic-regression-and-naive-bayes-models}{{6.4}{31}{Train Logistic Regression and Naive Bayes models}{section.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Display the predictions}{31}{section.6.5}}
\newlabel{display-the-predictions}{{6.5}{31}{Display the predictions}{section.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Evalue the modles with AUC and overall Accuracy}{33}{section.6.6}}
\newlabel{evalue-the-modles-with-auc-and-overall-accuracy}{{6.6}{33}{Evalue the modles with AUC and overall Accuracy}{section.6.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}A simple comparison on the two modles's performance}{33}{section.6.7}}
\newlabel{a-simple-comparison-on-the-two-modless-performance}{{6.7}{33}{A simple comparison on the two modles's performance}{section.6.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Overall Accuracy:}{33}{subsection.6.7.1}}
\newlabel{overall-accuracy}{{6.7.1}{33}{Overall Accuracy:}{subsection.6.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Area under ROC curve (AUC):}{33}{subsection.6.7.2}}
\newlabel{area-under-roc-curve-auc}{{6.7.2}{33}{Area under ROC curve (AUC):}{subsection.6.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.3}In summary, the Logistic Regression classifier has much better performance over the Naive Bayes, whatever on this perticular training/test set or potential future test datasets.}{33}{subsection.6.7.3}}
\newlabel{in-summary-the-logistic-regression-classifier-has-much-better-performance-over-the-naive-bayes-whatever-on-this-perticular-trainingtest-set-or-potential-future-test-datasets.}{{6.7.3}{33}{In summary, the Logistic Regression classifier has much better performance over the Naive Bayes, whatever on this perticular training/test set or potential future test datasets}{subsection.6.7.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Question 3 Part 3: Wikipedia Dataset with Logistic Regression and NaiveBayes classification}{34}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{question-3-part-3-wikipedia-dataset-with-logistic-regression-and-naivebayes-classification}{{7}{34}{Question 3 Part 3: Wikipedia Dataset with Logistic Regression and NaiveBayes classification}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Import required labraries}{34}{section.7.1}}
\newlabel{import-required-labraries}{{7.1}{34}{Import required labraries}{section.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Read the raw data and assigne lables with clustering results by K-MEANS}{34}{section.7.2}}
\newlabel{read-the-raw-data-and-assigne-lables-with-clustering-results-by-k-means}{{7.2}{34}{Read the raw data and assigne lables with clustering results by K-MEANS}{section.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Cast assigned lables to double and store to DataFrame}{35}{section.7.3}}
\newlabel{cast-assigned-lables-to-double-and-store-to-dataframe}{{7.3}{35}{Cast assigned lables to double and store to DataFrame}{section.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Split data into Training (80\%) and Test (20\%) datasets}{35}{section.7.4}}
\newlabel{split-data-into-training-80-and-test-20-datasets}{{7.4}{35}{Split data into Training (80\%) and Test (20\%) datasets}{section.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Config the pipeines for Logistic Regression and Naive Bayes}{36}{section.7.5}}
\newlabel{config-the-pipeines-for-logistic-regression-and-naive-bayes}{{7.5}{36}{Config the pipeines for Logistic Regression and Naive Bayes}{section.7.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Train Logistic Regression and Naive Bayes models}{36}{section.7.6}}
\newlabel{train-logistic-regression-and-naive-bayes-models}{{7.6}{36}{Train Logistic Regression and Naive Bayes models}{section.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Display predictions on the test data}{36}{section.7.7}}
\newlabel{display-predictions-on-the-test-data}{{7.7}{36}{Display predictions on the test data}{section.7.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Evaluate the models with Area under ROC curve (AUC) and overall Accuracy}{37}{section.7.8}}
\newlabel{evaluate-the-models-with-area-under-roc-curve-auc-and-overall-accuracy}{{7.8}{37}{Evaluate the models with Area under ROC curve (AUC) and overall Accuracy}{section.7.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.9}A simple comparison on the two modles's performance}{37}{section.7.9}}
\newlabel{a-simple-comparison-on-the-two-modless-performance}{{7.9}{37}{A simple comparison on the two modles's performance}{section.7.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.1}Overall Accuracy:}{37}{subsection.7.9.1}}
\newlabel{overall-accuracy}{{7.9.1}{37}{Overall Accuracy:}{subsection.7.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.2}Area under ROC curve (AUC):}{38}{subsection.7.9.2}}
\newlabel{area-under-roc-curve-auc}{{7.9.2}{38}{Area under ROC curve (AUC):}{subsection.7.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.3}In summary, the Logistic Regression classifier has much better performance over the Naive Bayes, whatever on this perticular training/test set or potential future test datasets.}{38}{subsection.7.9.3}}
\newlabel{in-summary-the-logistic-regression-classifier-has-much-better-performance-over-the-naive-bayes-whatever-on-this-perticular-trainingtest-set-or-potential-future-test-datasets.}{{7.9.3}{38}{In summary, the Logistic Regression classifier has much better performance over the Naive Bayes, whatever on this perticular training/test set or potential future test datasets}{subsection.7.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.9.4}\textbf  {In our raw data showed at the beginning, one class had much more data points (4423) than the other class (154). This bias significantly affected Naive Bayes classifier -\/- it had very hight error costs (false positive and false negative cost), but not the Logistic Regression classifier. So in our experiment, at least we can conclude that under this kind of bias, we should avoid using Naive Bayes classifier.}}{38}{subsection.7.9.4}}
\newlabel{in-our-raw-data-showed-at-the-beginning-one-class-had-much-more-data-points-4423-than-the-other-class-154.-this-bias-significantly-affected-naive-bayes-classifier----it-had-very-hight-error-costs-false-positive-and-false-negative-cost-but-not-the-logistic-regression-classifier.-so-in-our-experiment-at-least-we-can-conclude-that-under-this-kind-of-bias-we-should-avoid-using-naive-bayes-classifier.}{{7.9.4}{38}{\texorpdfstring {\textbf {In our raw data showed at the beginning, one class had much more data points (4423) than the other class (154). This bias significantly affected Naive Bayes classifier -\/- it had very hight error costs (false positive and false negative cost), but not the Logistic Regression classifier. So in our experiment, at least we can conclude that under this kind of bias, we should avoid using Naive Bayes classifier.}}{In our raw data showed at the beginning, one class had much more data points (4423) than the other class (154). This bias significantly affected Naive Bayes classifier -\/- it had very hight error costs (false positive and false negative cost), but not the Logistic Regression classifier. So in our experiment, at least we can conclude that under this kind of bias, we should avoid using Naive Bayes classifier.}}{subsection.7.9.4}{}}
